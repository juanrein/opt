{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data-driven optimization and decision making - Assignment 3\n",
    "Juha Reinikainen\n",
    "\n",
    "Use EI and the mean prediction to solve any single objective benchmark problem (e.g.\n",
    "Ackley, Rosenblock, sphere etc.) with any single objective optimizer (preferably GA). Set\n",
    "max exact function evaluations to 50 (start with 50 design points). Was the solutions\n",
    "found by EI better? (you can implement EI is you wish to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "from pymoo.factory import get_problem, get_visualization\n",
    "from scipy.stats import qmc\n",
    "from pymoo.algorithms.soo.nonconvex.ga import GA\n",
    "from pymoo.optimize import minimize\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from pymoo.core.problem import Problem\n",
    "from scipy.stats import norm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpectedImprovement(Problem):\n",
    "    def __init__(self, p: Problem, model, fxb, xi = 1e-5):\n",
    "        \"\"\"\n",
    "        p: problem object\n",
    "        model: model object\n",
    "        fxb: best objective function value found\n",
    "        xi: a parameter controlling the degree of exploration\n",
    "        \"\"\"\n",
    "        super().__init__(n_var=p.n_var, n_obj=1,\n",
    "                         n_constr=0, xl=p.xl, xu=p.xu)\n",
    "        self.model = model\n",
    "        self.fxb = fxb\n",
    "        self.xi = xi\n",
    "\n",
    "    def _evaluate(self, X, out, *args, **kwargs):\n",
    "        x_mean, x_std = self.model.predict(X, return_std=True)\n",
    "        x_var = np.sqrt(x_std).reshape(-1, 1)\n",
    "        # expected improvement\n",
    "        inner = (self.fxb - x_mean - self.xi)/x_var\n",
    "        left = (self.fxb - x_mean - self.xi) * norm.cdf(inner)\n",
    "        right = x_var * norm.pdf(inner)\n",
    "        EI = left + right\n",
    "        # negate to fit the interface requiring minimization\n",
    "        out[\"F\"] = -EI\n",
    "\n",
    "class Surrogate(Problem):\n",
    "    \"\"\"\n",
    "    Surrogate to Problem wrapper\n",
    "    \"\"\"\n",
    "    def __init__(self, p, model):\n",
    "        \"\"\"\n",
    "        p: problem object\n",
    "        model: surrogate model\n",
    "        \"\"\"\n",
    "        super().__init__(n_var=p.n_var, n_obj=1,\n",
    "                         n_constr=0, xl=p.xl, xu=p.xu)\n",
    "        self.model = model\n",
    "\n",
    "    def _evaluate(self, X, out, *args, **kwargs):\n",
    "        out[\"F\"] = self.model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def online_datadriven_rosenbrock_optimization(use_EI = False):\n",
    "    \"\"\"\n",
    "    find minimum of rosenbrock\n",
    "    use_EI use expected improvement strategy to choose point to evaluate\n",
    "    otherwise mean approximation is used\n",
    "    \"\"\"\n",
    "    seed = 1\n",
    "    max_eval = 50\n",
    "    # rosenbrock = get_problem(\"ackley\", n_var=1, a=20, b=1/5, c=2 * np.pi)\n",
    "    #x = 1, f = 0\n",
    "    rosenbrock = get_problem(\"rosenbrock\", n_var=2)\n",
    "    surrogate = GaussianProcessRegressor()\n",
    "    \n",
    "    optimizer = GA(pop_size=100)\n",
    "    optimizer_acquisition = GA(pop_size=100)\n",
    "\n",
    "    #generate initial sample\n",
    "    lhs = qmc.LatinHypercube(rosenbrock.n_var, seed=seed)\n",
    "    X = lhs.random(50)\n",
    "    X = qmc.scale(X, rosenbrock.xl, rosenbrock.xu)\n",
    "    y = rosenbrock.evaluate(X)\n",
    "\n",
    "    #Build surrogates with initial data\n",
    "    surrogate.fit(X, y)\n",
    "\n",
    "    history = {\n",
    "        \"X\": [],\n",
    "        \"y\": []\n",
    "    }\n",
    "\n",
    "    surrogateProblem = Surrogate(rosenbrock, surrogate)\n",
    "    for _ in range(max_eval):\n",
    "        #find optimum of surrogate\n",
    "        best = minimize(surrogateProblem, optimizer, (\"n_gen\", 10), seed=seed)\n",
    "        history[\"X\"].append(best.X)\n",
    "        history[\"y\"].append(best.F)\n",
    "\n",
    "        #choose which point to evaluate\n",
    "        if use_EI:\n",
    "            #construct EI rosenbrock with found optimum\n",
    "            ei = ExpectedImprovement(rosenbrock, surrogate, best.F)\n",
    "\n",
    "            #optimize considering expected improvement\n",
    "            res = minimize(ei, optimizer_acquisition, (\"n_gen\", 10), seed=seed)\n",
    "            x_t = res.X\n",
    "        else:\n",
    "            x_t = best.X\n",
    "\n",
    "        #print(res.X, -res.F, end = \", \")\n",
    "        #evaluate the chosen point with real function\n",
    "        y_t = rosenbrock.evaluate(x_t)\n",
    "\n",
    "        #add newly evaluated to data\n",
    "        X = np.append(X, [x_t], axis=0)\n",
    "        y = np.append(y, [y_t], axis=0)\n",
    "        #Rebuild the surrogates with the new data\n",
    "        surrogate.fit(X, y)\n",
    "\n",
    "    # do final optimization\n",
    "    best = minimize(surrogateProblem, optimizer, (\"n_gen\", 10), seed=seed)\n",
    "    history[\"X\"].append(best.X)\n",
    "    history[\"y\"].append(best.F)\n",
    "\n",
    "    return best.X, best.F, history\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1,y1, hist1 = online_datadriven_rosenbrock_optimization(False)\n",
    "x2,y2, hist2 = online_datadriven_rosenbrock_optimization(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean [0.97923458 0.95661389] [0.00099707] 0.04809944780227716 [0.00099707]\n",
      "EI   [1.05692369 1.1119617 ] [0.03909372] 0.12560146341718723 [0.03909372]\n"
     ]
    }
   ],
   "source": [
    "x_true = np.ones(x1.shape)\n",
    "y_true = np.zeros(y1.shape)\n",
    "print(\"mean\", x1,y1, np.linalg.norm(x_true - x1), np.abs(y_true - y1))\n",
    "print(\"EI  \", x2,y2, np.linalg.norm(x_true - x2), np.abs(y_true - y2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean prediction is a little better than expected improvement on 2 dimensional Rosenbrock function. "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d67f4bcf1604c44bf18ec22d97f283eada189abc7af111a58bd3017a8979d250"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
