{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data-driven optimization and decision making - Assignment 2\n",
    "Juha Reinikainen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import qmc\n",
    "from desdeo_problem import variable_builder, \\\n",
    "    ScalarConstraint, MOProblem, ScalarObjective\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor, kernels\n",
    "from sklearn.model_selection import cross_validate, GridSearchCV\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the problem\n",
    "# https://www.mathworks.com/help/gads/multiobjective-optimization-welded-beam.html\n",
    "def F1(x):\n",
    "    \"\"\"\n",
    "    minimize the fabrication cost of the beam\n",
    "    \"\"\"\n",
    "    return 1.10471 * x[:, 0]**2 * x[:, 1] + 0.04811 * x[:, 2] * x[:, 3] * (14.0 + x[:, 1])\n",
    "\n",
    "\n",
    "P = 6000.0  # max supported load of the beam\n",
    "L = 14.0  # length of part\n",
    "\n",
    "\n",
    "def F2(x):\n",
    "    \"\"\"\n",
    "    Minimize the deflection of the beam\n",
    "    \"\"\"\n",
    "    C = 4*(14**3)/(30*10**6)\n",
    "    return (P / (x[:, 3] * x[:, 2]**3)) * C\n",
    "\n",
    "\n",
    "objectives = [\n",
    "    ScalarObjective(\"cost\", F1, maximize=False),\n",
    "    ScalarObjective(\"deflection\", F2, maximize=False)\n",
    "]\n",
    "\n",
    "\"\"\"\n",
    "Welded beam design problem\n",
    "x[0] the thickness of the welds\n",
    "x[1] the length of the welds\n",
    "x[2] the height of the beam\n",
    "x[3] the width of the beam\n",
    "\"\"\"\n",
    "var_names = [\"thickness\", \"length\", \"height\", \"width\"]\n",
    "lb = [0.125, 0.1, 0.1, 0.125]\n",
    "ub = [5.0, 10.0, 10.0, 5.0]\n",
    "initial_values = [0.125, 0.1, 0.1, 0.125]\n",
    "variables = variable_builder(var_names, initial_values, lb, ub)\n",
    "\n",
    "\n",
    "def shear_stress(x, y):\n",
    "    r1 = 1 / np.sqrt(2 * x[:, 0] * x[:, 1])\n",
    "    R = np.sqrt(x[:, 1]**2 + (x[:, 0] + x[:, 2])**2)\n",
    "    r2 = ((L + x[:, 1]/2) * R) / np.sqrt(2 * x[:, 0] *\n",
    "                                         x[:, 2] * ((x[:, 1]**2)/3 + (x[:, 0] + x[:, 2])**2))\n",
    "    r = P * np.sqrt(r1**2 + r2**2 + (2*r1*r2*x[:, 1])/R)\n",
    "    return 13600 - r\n",
    "\n",
    "\n",
    "constraints = [\n",
    "    ScalarConstraint(\"thickness\", 4, 2, lambda x, y: x[:, 3] - x[:, 0]),\n",
    "    ScalarConstraint(\"shear stress\", 4, 2, shear_stress),\n",
    "    ScalarConstraint(\"normal stress\", 4, 2, lambda x,\n",
    "                     y: 30000 - P * (6 * L)/(x[:, 3]*x[:, 2]**2)),\n",
    "    ScalarConstraint(\"buckling load\", 4, 2, lambda x, y: (\n",
    "        64746.022 * (1-0.0282346*x[:, 2])*x[:, 2]*x[:, 3]**3) - 6000),\n",
    "]\n",
    "\n",
    "prob = MOProblem(objectives=objectives, variables=variables,\n",
    "                 constraints=constraints)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate samples and scale them based on the box constraints\n",
    "lhs = qmc.LatinHypercube(4, seed=seed)\n",
    "X = lhs.random(400)\n",
    "X = qmc.scale(X, lb, ub)\n",
    "res = prob.evaluate(X)\n",
    "y = res.objectives\n",
    "\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# scaler = MinMaxScaler()\n",
    "# y = scaler.fit_transform(y)\n",
    "\n",
    "# class Objective2Transformer(TransformerMixin):\n",
    "#     def fit_transform(self, X, y = None, **fit_params):\n",
    "#         return X[:,2:]\n",
    "\n",
    "y1 = y[:, 0]\n",
    "y2 = y[:, 1]\n",
    "# print(y1.min(), y1.max())\n",
    "# print(y2.min(), y2.max())\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.scatter([yi for yi,c in zip(y1, res.constraints) if (c >= 0).all()], \\\n",
    "#     [yi for yi,c in zip(y2, res.constraints) if (c >= 0).all()])\n",
    "# plt.show()\n",
    "# fix, axs = plt.subplots(2)\n",
    "# axs[0].hist(y1)\n",
    "# axs[1].hist(y2)\n",
    "# plt.show()\n",
    "\n",
    "# TODO: normalize range of objectives\n",
    "# TODO: shuffle? approximation error in different areas?\n",
    "# TODO: ScalarDataObjective\n",
    "# TODO: approximating areas where solutions are unfeasible is not neccessary\n",
    "# TODO: find another version of problem description\n",
    "# TODO: maybe do multiple runs\n",
    "# TODO: the second objective on dimends on two variables\n",
    "# TODO: use same score metrics for tuning as crossvalidation?\n",
    "\n",
    "# test samples\n",
    "X_test = lhs.random(50)\n",
    "X_test = qmc.scale(X_test, lb, ub)\n",
    "\n",
    "res_test = prob.evaluate(X_test)\n",
    "y_test = res_test.objectives\n",
    "y1_test = y_test[:, 0]\n",
    "y2_test = y_test[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective 1\n",
      "r2 [0.966 0.971 0.973 0.972 0.976] mean 0.9716673298402456 std 0.0034609644379274174\n",
      "rmse [-9.678 -10.321 -8.807 -9.285 -8.588] mean -9.335781605503602 std 0.6211121440238567\n",
      "objective 2\n",
      "r2 [0.714 0.555 -4.590 0.980 0.565] mean -0.3554313269698042 std 2.122952476723004\n",
      "rmse [-33.745 -101.239 -35.096 -7.949 -56.505] mean -46.90686182976308 std 31.223808842493536\n"
     ]
    }
   ],
   "source": [
    "randomForestRegressor_y1 = RandomForestRegressor()\n",
    "randomForestRegressor_y1.fit(X, y1)\n",
    "randomForestRegressor_y2 = RandomForestRegressor()\n",
    "randomForestRegressor_y2.fit(X[:, 2:], y2)\n",
    "# randomForestRegressor.fit(X,y)\n",
    "# sklearn only has negative rmse for some reason\n",
    "# min rmse, max r2\n",
    "scoring = [\"r2\", \"neg_root_mean_squared_error\"]\n",
    "# 5-folds\n",
    "scores_y1 = cross_validate(randomForestRegressor_y1,\n",
    "                           X, y1, scoring=scoring, cv=5)\n",
    "scores_y2 = cross_validate(randomForestRegressor_y2,\n",
    "                           X[:, 2:], y2, scoring=scoring, cv=5)\n",
    "print(\"objective 1\")\n",
    "print(\"r2\", scores_y1[\"test_r2\"], \"mean\", scores_y1[\"test_r2\"].mean(\n",
    "), \"std\", scores_y1[\"test_r2\"].std())\n",
    "print(\"rmse\", scores_y1[\"test_neg_root_mean_squared_error\"],\n",
    "      \"mean\", scores_y1[\"test_neg_root_mean_squared_error\"].mean(),\n",
    "      \"std\", scores_y1[\"test_neg_root_mean_squared_error\"].std())\n",
    "\n",
    "print(\"objective 2\")\n",
    "print(\"r2\", scores_y2[\"test_r2\"], \"mean\", scores_y2[\"test_r2\"].mean(\n",
    "), \"std\", scores_y2[\"test_r2\"].std())\n",
    "print(\"rmse\", scores_y2[\"test_neg_root_mean_squared_error\"],\n",
    "      \"mean\", scores_y2[\"test_neg_root_mean_squared_error\"].mean(),\n",
    "      \"std\", scores_y2[\"test_neg_root_mean_squared_error\"].std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Juha Reinikainen\\Documents\\dataopt\\env\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:610: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\Juha Reinikainen\\Documents\\dataopt\\env\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:610: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\Juha Reinikainen\\Documents\\dataopt\\env\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 1000000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Juha Reinikainen\\Documents\\dataopt\\env\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter periodicity is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Juha Reinikainen\\Documents\\dataopt\\env\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 1000000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Juha Reinikainen\\Documents\\dataopt\\env\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter periodicity is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Juha Reinikainen\\Documents\\dataopt\\env\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 1000000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Juha Reinikainen\\Documents\\dataopt\\env\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter periodicity is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Juha Reinikainen\\Documents\\dataopt\\env\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 1000000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Juha Reinikainen\\Documents\\dataopt\\env\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter periodicity is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Juha Reinikainen\\Documents\\dataopt\\env\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified upper bound 1000000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Juha Reinikainen\\Documents\\dataopt\\env\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter periodicity is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Juha Reinikainen\\Documents\\dataopt\\env\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter periodicity is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Juha Reinikainen\\Documents\\dataopt\\env\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter periodicity is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Juha Reinikainen\\Documents\\dataopt\\env\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:610: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\Juha Reinikainen\\Documents\\dataopt\\env\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-10. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Juha Reinikainen\\Documents\\dataopt\\env\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-10. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Juha Reinikainen\\Documents\\dataopt\\env\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-10. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Juha Reinikainen\\Documents\\dataopt\\env\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-10. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Juha Reinikainen\\Documents\\dataopt\\env\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-10. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Juha Reinikainen\\Documents\\dataopt\\env\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-10. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Juha Reinikainen\\Documents\\dataopt\\env\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter periodicity is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Juha Reinikainen\\Documents\\dataopt\\env\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-10. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Juha Reinikainen\\Documents\\dataopt\\env\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter periodicity is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Juha Reinikainen\\Documents\\dataopt\\env\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-10. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Juha Reinikainen\\Documents\\dataopt\\env\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter periodicity is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Juha Reinikainen\\Documents\\dataopt\\env\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-10. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Juha Reinikainen\\Documents\\dataopt\\env\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter periodicity is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Juha Reinikainen\\Documents\\dataopt\\env\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-10. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Juha Reinikainen\\Documents\\dataopt\\env\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter periodicity is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Juha Reinikainen\\Documents\\dataopt\\env\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-10. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Juha Reinikainen\\Documents\\dataopt\\env\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-10. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Juha Reinikainen\\Documents\\dataopt\\env\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-10. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Juha Reinikainen\\Documents\\dataopt\\env\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-10. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Juha Reinikainen\\Documents\\dataopt\\env\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-10. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Juha Reinikainen\\Documents\\dataopt\\env\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-10. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Juha Reinikainen\\Documents\\dataopt\\env\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter periodicity is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Juha Reinikainen\\Documents\\dataopt\\env\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-10. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Juha Reinikainen\\Documents\\dataopt\\env\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter periodicity is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Juha Reinikainen\\Documents\\dataopt\\env\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-10. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Juha Reinikainen\\Documents\\dataopt\\env\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter periodicity is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Juha Reinikainen\\Documents\\dataopt\\env\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-10. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Juha Reinikainen\\Documents\\dataopt\\env\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter periodicity is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Juha Reinikainen\\Documents\\dataopt\\env\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter periodicity is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Juha Reinikainen\\Documents\\dataopt\\env\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter periodicity is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Juha Reinikainen\\Documents\\dataopt\\env\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-10. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Juha Reinikainen\\Documents\\dataopt\\env\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter periodicity is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianProcessRegressor(kernel=ExpSineSquared(length_scale=1, periodicity=1),\n",
      "                         n_restarts_optimizer=1, normalize_y=True,\n",
      "                         random_state=42)\n",
      "GaussianProcessRegressor(alpha=0.1,\n",
      "                         kernel=ExpSineSquared(length_scale=1, periodicity=1),\n",
      "                         n_restarts_optimizer=1, normalize_y=True,\n",
      "                         random_state=42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Juha Reinikainen\\Documents\\dataopt\\env\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:610: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\Juha Reinikainen\\Documents\\dataopt\\env\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-10. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Juha Reinikainen\\Documents\\dataopt\\env\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter periodicity is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Juha Reinikainen\\Documents\\dataopt\\env\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-10. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Juha Reinikainen\\Documents\\dataopt\\env\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter periodicity is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Juha Reinikainen\\Documents\\dataopt\\env\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-10. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Juha Reinikainen\\Documents\\dataopt\\env\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter periodicity is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Juha Reinikainen\\Documents\\dataopt\\env\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-10. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Juha Reinikainen\\Documents\\dataopt\\env\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter periodicity is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective 1\n",
      "r2 [1.000 1.000 1.000 1.000 1.000] mean 0.9999953044730745 std 1.2140493896679959e-06\n",
      "rmse [-0.106 -0.115 -0.113 -0.116 -0.148] mean -0.11946919371550106 std 0.014554306896624177\n",
      "objective 2\n",
      "r2 [-0.000 0.534 -0.440 -0.003 -0.002] mean 0.01775180655793025 std 0.30887067035507587\n",
      "rmse [-63.107 -103.614 -17.812 -55.821 -85.690] mean -65.20881028181434 std 29.097886498310366\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    \"alpha\": [1e-10, 1e-1, 1, 10], #[1e-10, 1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1e-0, 1e1, 1e2, 1e3, 1e4],\n",
    "    #\"normalize_y\": [True, False],\n",
    "    \"kernel\": [\n",
    "        kernels.RBF(length_scale_bounds=(1e-10, 1000000)),\n",
    "        kernels.ExpSineSquared(length_scale_bounds=(1e-10, 1000000))\n",
    "    ]\n",
    "}\n",
    "\n",
    "gaussianProcessRegressor_y1 = \\\n",
    "    GridSearchCV(GaussianProcessRegressor(normalize_y=True,\n",
    "        n_restarts_optimizer=1, random_state=seed), parameters)\n",
    "gaussianProcessRegressor_y2 = \\\n",
    "    GridSearchCV(GaussianProcessRegressor(normalize_y=True,\n",
    "        n_restarts_optimizer=1, random_state=seed), parameters)\n",
    "# fit best parameters\n",
    "gaussianProcessRegressor_y1.fit(X, y1)\n",
    "gaussianProcessRegressor_y1 = gaussianProcessRegressor_y1.best_estimator_\n",
    "gaussianProcessRegressor_y2.fit(X[:, 2:], y2)\n",
    "gaussianProcessRegressor_y2 = gaussianProcessRegressor_y2.best_estimator_\n",
    "print(gaussianProcessRegressor_y1)\n",
    "print(gaussianProcessRegressor_y2)\n",
    "\n",
    "# 5-folds\n",
    "scores_y1 = cross_validate(gaussianProcessRegressor_y1,\n",
    "                           X, y1, scoring=scoring, cv=5)\n",
    "scores_y2 = cross_validate(gaussianProcessRegressor_y2,\n",
    "                           X[:, 2:], y2, scoring=scoring, cv=5)\n",
    "\n",
    "print(\"objective 1\")\n",
    "print(\"r2\", scores_y1[\"test_r2\"], \"mean\", scores_y1[\"test_r2\"].mean(\n",
    "), \"std\", scores_y1[\"test_r2\"].std())\n",
    "print(\"rmse\", scores_y1[\"test_neg_root_mean_squared_error\"],\n",
    "      \"mean\", scores_y1[\"test_neg_root_mean_squared_error\"].mean(),\n",
    "      \"std\", scores_y1[\"test_neg_root_mean_squared_error\"].std())\n",
    "\n",
    "print(\"objective 2\")\n",
    "print(\"r2\", scores_y2[\"test_r2\"], \"mean\", scores_y2[\"test_r2\"].mean(\n",
    "), \"std\", scores_y2[\"test_r2\"].std())\n",
    "print(\"rmse\", scores_y2[\"test_neg_root_mean_squared_error\"],\n",
    "      \"mean\", scores_y2[\"test_neg_root_mean_squared_error\"].mean(),\n",
    "      \"std\", scores_y2[\"test_neg_root_mean_squared_error\"].std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random forest test\n",
      "r2 objective 1 0.9820018939766092\n",
      "rmse objective 1 8.355749526953662\n",
      "r2 objective 2 0.9726699928742291\n",
      "rmse objective 2 3.214175671694415\n",
      "kriging test\n",
      "r2 objective 1 0.9999958210149814\n",
      "rmse objective 1 0.12732316249506315\n",
      "r2 objective 2 -0.12101709512524295\n",
      "rmse objective 2 20.585236213432715\n"
     ]
    }
   ],
   "source": [
    "randomForestRegressor_y1.fit(X, y1)\n",
    "randomForestRegressor_y2.fit(X[:, 2:], y2)\n",
    "y1_pred_forest = randomForestRegressor_y1.predict(X_test)\n",
    "y2_pred_forest = randomForestRegressor_y2.predict(X_test[:, 2:])\n",
    "print(\"random forest test\")\n",
    "print(\"r2 objective 1\", r2_score(y1_test, y1_pred_forest))\n",
    "print(\"rmse objective 1\", mean_squared_error(\n",
    "    y1_test, y1_pred_forest, squared=False))\n",
    "print(\"r2 objective 2\", r2_score(y2_test, y2_pred_forest))\n",
    "print(\"rmse objective 2\", mean_squared_error(\n",
    "    y2_test, y2_pred_forest, squared=False))\n",
    "\n",
    "y1_pred_kriging = gaussianProcessRegressor_y1.predict(X_test)\n",
    "y2_pred_kriging = gaussianProcessRegressor_y2.predict(X_test[:, 2:])\n",
    "print(\"kriging test\")\n",
    "print(\"r2 objective 1\", r2_score(y1_test, y1_pred_kriging))\n",
    "print(\"rmse objective 1\", mean_squared_error(\n",
    "    y1_test, y1_pred_kriging, squared=False))\n",
    "print(\"r2 objective 2\", r2_score(y2_test, y2_pred_kriging))\n",
    "print(\"rmse objective 2\", mean_squared_error(\n",
    "    y2_test, y2_pred_kriging, squared=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|  | | Objective 1 | | Objective 2 | |\n",
    "| ---- | ---- | -- | -- | -- | -- |\n",
    "|  | | Kriging | Random forest | Kriging | Random forest |\n",
    "| k-fold CV | R-squared | | | | |\n",
    "|  | RMSE | | | | |\n",
    "| Test | R-squared | | | | |\n",
    "|  | RMSE | | | | |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[28.330 49.905 64.355 96.606 89.047 35.097 147.490 25.896 2.682 58.826]\n",
      " [28.339 58.063 67.037 116.896 83.067 27.356 154.680 29.046 2.234 42.012]\n",
      " [28.339 58.311 67.051 116.965 83.068 27.362 154.663 29.033 2.230 42.012]]\n",
      "[[0.002 33.004 0.018 0.002 0.002 0.532 0.002 0.001 0.271 0.157]\n",
      " [10.203 10.203 10.203 10.203 10.203 10.203 10.203 10.203 10.203 10.203]\n",
      " [0.002 11.297 0.016 0.001 0.003 0.413 0.002 0.001 0.623 0.113]]\n"
     ]
    }
   ],
   "source": [
    "# gaussian process inaccurate in large >100 y1 range?\n",
    "np.set_printoptions(formatter={\"all\": lambda s: f\"{s:.3f}\"})\n",
    "print(np.array([\n",
    "    randomForestRegressor_y1.predict(X_test[:10]),\n",
    "    gaussianProcessRegressor_y1.predict(X_test[:10]),\n",
    "    y1_test[:10]\n",
    "]))\n",
    "print(np.array([\n",
    "    randomForestRegressor_y2.predict(X_test[:10, 2:]),\n",
    "    gaussianProcessRegressor_y2.predict(X_test[:10, 2:]),\n",
    "    y2_test[:10]\n",
    "]))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d67f4bcf1604c44bf18ec22d97f283eada189abc7af111a58bd3017a8979d250"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
